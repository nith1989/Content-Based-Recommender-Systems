{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:\n",
      "[('action', 0), ('adventure', 1), ('animation', 2), ('children', 3), ('comedy', 4), ('crime', 5), ('documentary', 6), ('drama', 7), ('fantasy', 8), ('film-noir', 9)]\n",
      "99903 training ratings; 101 testing ratings\n",
      "error=0.780817\n",
      "[ 2.7945653   2.60829366  2.76575085  4.26282897  3.26666444  4.13710176\n",
      "  3.92010483  3.96471874  3.19335333  3.39862982]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "#\n",
    "# Here we'll implement a content-based recommendation algorithm.\n",
    "# It will use the list of genres for a movie as the content.\n",
    "# The data come from the MovieLens project: http://grouplens.org/datasets/movielens/\n",
    "# Note that I have not provided many doctests for this one. I strongly\n",
    "# recommend that you write your own for each function to ensure your\n",
    "# implementation is correct.\n",
    "\n",
    "# Please only use these imports.\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def download_data():\n",
    "    \"\"\" DONE. Download and unzip data.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/h9ubx22ftdkyvd5/ml-latest-small.zip?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'ml-latest-small.zip')\n",
    "    zfile = zipfile.ZipFile('ml-latest-small.zip')\n",
    "    zfile.extractall()\n",
    "    zfile.close()\n",
    "\n",
    "\n",
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())\n",
    "\n",
    "\n",
    "def tokenize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the genre column from the dataframe & create a list of list with tokenization output\n",
    "    res=[]\n",
    "    \n",
    "    genre=movies['genres']\n",
    "    for e in genre:\n",
    "        token=e\n",
    "        res_tmp=tokenize_string(token)\n",
    "        res.append(res_tmp)\n",
    "    \n",
    "    # Create a new column in the movies dataframe  using the above list of lists\n",
    "    length = len(movies['genres'])\n",
    "    movies.loc[:,'tokens'] = pd.Series(res, index=movies.index)\n",
    "    \n",
    "    return movies\n",
    "\n",
    "\n",
    "def featurize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "    \"\"\"\n",
    "    # Get the list of genres for all movies\n",
    "    nmov = movies.movieId.nunique()\n",
    "    tokens=list(movies['tokens'])\n",
    "    \n",
    "    # Calculate number of unique documents of each of the genre\n",
    "    feats=[j for i in tokens for j in i]\n",
    "    feats_cnt=dict(Counter(feats))\n",
    "    res=[]\n",
    "    \n",
    "    # Looping through movies\n",
    "    for each in tokens:\n",
    "        \n",
    "        # Get frequency of all genres in this movie (to see if there are repeated genres)\n",
    "        cnt=dict(Counter(each))\n",
    "        \n",
    "        # Number of features for this document\n",
    "        n_feats=len(cnt)\n",
    "        \n",
    "        # Genre frequency for genre with maximum frequency\n",
    "        max_d=cnt[max(cnt,key=cnt.get)]\n",
    "            \n",
    "        # Create the TF & IDF score for each genre\n",
    "        for key, value in cnt.items():\n",
    "            df_i=feats_cnt[key]\n",
    "            cnt[key] = (value / max_d) * math.log10(nmov/df_i)\n",
    "        res.append(cnt)\n",
    "        \n",
    "    # Create vocab dictionary with the genre & column indices for each genre in alphabetic order\n",
    "    col_set=list(feats_cnt)\n",
    "    col_set=sorted(col_set)\n",
    "\n",
    "    vocab=[]\n",
    "    i=-1\n",
    "    for each in col_set:\n",
    "        i+=1\n",
    "        vocab.append((each,i))\n",
    "    vocab_final=dict(vocab)\n",
    "    \n",
    "    # Creating CSR matrix\n",
    "    ncol=len(vocab_final)\n",
    "    nrow=1\n",
    "    \n",
    "    mat_res=[]\n",
    "    \n",
    "    # Loop through each movie from the res dictionary\n",
    "    for each in res:\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        val=[]\n",
    "        row=0\n",
    "        # Loop through each genre of the movie\n",
    "        for e in each.items():\n",
    "            row_token=row\n",
    "            rows.append(row_token)\n",
    "        \n",
    "            col_token=vocab_final[e[0]]\n",
    "            cols.append(col_token)\n",
    "           \n",
    "            val_tmp=e[1]\n",
    "            val.append(val_tmp)\n",
    "        \n",
    "        row_final = np.array(rows)\n",
    "        col_final = np.array(cols)\n",
    "        data = np.array(val)\n",
    "        res_matrix=csr_matrix((data, (row_final, col_final)), shape=(nrow,ncol))\n",
    "        mat_res.append(res_matrix)\n",
    "        \n",
    "    # Storing the CSR matrix as a feature in the dataframe\n",
    "    length = len(movies['tokens'])\n",
    "    movies.loc[:,'features'] = pd.Series(mat_res, index=movies.index)\n",
    "        \n",
    "    return movies,vocab_final\n",
    "\n",
    "def train_test_split(ratings):\n",
    "    \"\"\"DONE.\n",
    "    Returns a random split of the ratings matrix into a training and testing set.\n",
    "    \"\"\"\n",
    "    test = set(range(len(ratings))[::1000])\n",
    "    train = sorted(set(range(len(ratings))) - test)\n",
    "    test = sorted(test)\n",
    "    return ratings.iloc[train], ratings.iloc[test]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two 1-d csr_matrices.\n",
    "    Each matrix represents the tf-idf feature vector of a movie.\n",
    "    Params:\n",
    "      a...A csr_matrix with shape (1, number_features)\n",
    "      b...A csr_matrix with shape (1, number_features)\n",
    "    Returns:\n",
    "      The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\n",
    "      where ||a|| indicates the Euclidean norm (aka L2 norm) of vector a.\n",
    "    \"\"\"\n",
    "    \n",
    "    am=a.toarray()[0]\n",
    "    bm=b.toarray()[0]\n",
    "    \n",
    "    nmrtr=np.dot(am,bm)\n",
    "    dnmtr=np.sqrt(np.dot(am,am))*np.sqrt(np.dot(bm,bm))\n",
    "    return nmrtr/dnmtr\n",
    "\n",
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    \"\"\"\n",
    "    Using the ratings in ratings_train, predict the ratings for each\n",
    "    row in ratings_test.\n",
    "\n",
    "    To predict the rating of user u for movie i: Compute the weighted average\n",
    "    rating for every other movie that u has rated.  Restrict this weighted\n",
    "    average to movies that have a positive cosine similarity with movie\n",
    "    i. The weight for movie m corresponds to the cosine similarity between m\n",
    "    and i.\n",
    "\n",
    "    If there are no other movies with positive cosine similarity to use in the\n",
    "    prediction, use the mean rating of the target user in ratings_train as the\n",
    "    prediction.\n",
    "\n",
    "    Params:\n",
    "      movies..........The movies DataFrame.\n",
    "      ratings_train...The subset of ratings used for making predictions. These are the \"historical\" data.\n",
    "      ratings_test....The subset of ratings that need to predicted. These are the \"future\" data.\n",
    "    Returns:\n",
    "      A numpy array containing one predicted rating for each element of ratings_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all the movies in test dataframe\n",
    "    tmovies=ratings_test['movieId'].unique()\n",
    "    \n",
    "    # Get the information of CSR matrix from main movie data frame for the test & train movies\n",
    "    tmovies_all=movies[movies['movieId'].isin(tmovies)]\n",
    "    trmovies_all=movies[-movies['movieId'].isin(tmovies)]\n",
    "    \n",
    "    sim_movies={}\n",
    "        \n",
    "    # Create a master similarity list for every test movie with every train movie when similarity>0\n",
    "    for u,v in tmovies_all.iterrows():\n",
    "        cur_m=v.movieId\n",
    "        cur_csr=v.features\n",
    "           \n",
    "        # Loop through every other movie to obtain similarity\n",
    "        for x,y in trmovies_all.iterrows():\n",
    "                nex_m=y.movieId\n",
    "                nex_csr=y.features\n",
    "                mtmp=sorted(tuple((cur_m,nex_m)))\n",
    "                s=str(mtmp[0])+','+str(mtmp[1])\n",
    "                sim=cosine_sim(cur_csr,nex_csr)\n",
    "                if sim>0:\n",
    "                    sim_movies.update({s:sim})\n",
    "    \n",
    "    \n",
    "    result=[]\n",
    "    \n",
    "    # Create a list of lists from the above list of tuples keeping only the movie IDs\n",
    "    # For every user & movie combination in the test data\n",
    "    for p,q in ratings_test.iterrows():\n",
    "        tuser=int(q.userId)\n",
    "        tmovie=int(q.movieId)\n",
    "        \n",
    "        # Get the list of movies from training data that this user has rated and movies which have a similarity>0\n",
    "        umovies_tmp=ratings_train[ratings_train.userId==tuser]['movieId']\n",
    "        \n",
    "        # Create a list of lists where each sublist is a tuple of the movie i and one movie that this user rated\n",
    "        stmp_f=[]\n",
    "        stmp_f1=[]\n",
    "        \n",
    "        for e in umovies_tmp:\n",
    "            stmp=sorted(tuple((tmovie,e)))\n",
    "            t=str(stmp[0])+','+str(stmp[1])\n",
    "            stmp_f.append(t)\n",
    "            stmp_f1.append(stmp)\n",
    "           \n",
    "        # Get the subset of ratings list for movies present above\n",
    "        # Get subset of movies that are similar to current movie\n",
    "        mlist_f=[]\n",
    "        for e in stmp_f1:\n",
    "            if (str(e[0])+','+str(e[1])) in sim_movies.keys():\n",
    "                mlist_f.append(e)     \n",
    "        clist=[j for i in mlist_f for j in i]   \n",
    "        clist_f=[c for c in clist if c!=tmovie]\n",
    "        \n",
    "        # Filter the user ratings data for the above movies\n",
    "        umovies1=ratings_train[ratings_train.userId==tuser]\n",
    "        umovies2=umovies1[umovies1['movieId'].isin(clist_f)]\n",
    "         \n",
    "        shp=umovies2.shape[0]\n",
    "        umovies3=umovies2.copy()\n",
    "            \n",
    "        # If we find similar movies then\n",
    "        if shp>0:\n",
    "        \n",
    "            sim_f=[]\n",
    "            # Add the similarity score as a column in the above dataframe\n",
    "            for i,r in umovies2.iterrows():\n",
    "                g=int(r.movieId)\n",
    "                stmp=sorted(tuple((tmovie,g)))\n",
    "                t=str(stmp[0])+','+str(stmp[1])\n",
    "                sim=sim_movies[t]\n",
    "                sim_f.append(sim)\n",
    "        \n",
    "                \n",
    "            umovies3['sim_score'] = pd.Series(sim_f, index=umovies3.index)\n",
    "            \n",
    "            # Calculating weighted average for each user for the movie\n",
    "            wght=np.array(umovies3.sim_score)\n",
    "            rate=np.array(umovies3.rating)\n",
    "            \n",
    "            nmrtr=np.dot(wght,rate)\n",
    "            dnmtr=np.sum(wght)\n",
    "            \n",
    "            pred=nmrtr/dnmtr\n",
    "            result.append(pred)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Add the regular average of user's ratings for all movies\n",
    "            rate=np.array(umovies1.rating)\n",
    "            \n",
    "            nmrtr=np.sum(rate)\n",
    "            dnmtr=len(umovies1.movieId)\n",
    "            \n",
    "            pred=nmrtr/dnmtr\n",
    "            result.append(pred)\n",
    "    \n",
    "    res=np.array(result)\n",
    "    return res\n",
    "\n",
    "\n",
    "def mean_absolute_error(predictions, ratings_test):\n",
    "    \"\"\"DONE.\n",
    "    Return the mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    return np.abs(predictions - np.array(ratings_test.rating)).mean()\n",
    "\n",
    "\n",
    "def main():\n",
    "    download_data()\n",
    "    path = 'ml-latest-small'\n",
    "    ratings = pd.read_csv(path + os.path.sep + 'ratings.csv')\n",
    "    movies = pd.read_csv(path + os.path.sep + 'movies.csv')\n",
    "    movies = tokenize(movies)\n",
    "    movies, vocab = featurize(movies)\n",
    "    print('vocab:')\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "    print('error=%f' % mean_absolute_error(predictions, ratings_test))\n",
    "    print(predictions[:10])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
